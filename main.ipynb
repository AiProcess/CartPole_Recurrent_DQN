{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchrl.envs import (\n",
    "    GymEnv,\n",
    "    Compose,\n",
    "    TransformedEnv,\n",
    "    ToTensorImage,\n",
    "    GrayScale,\n",
    "    StepCounter,\n",
    "    Resize,\n",
    "    InitTracker,\n",
    "    RewardScaling,\n",
    "    ObservationNorm,\n",
    "    set_exploration_type,\n",
    "    ExplorationType\n",
    ")\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "import matplotlib.pyplot as plt\n",
    "from torchrl.modules import ConvNet, EGreedyModule, LSTMModule, MLP, QValueModule\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Selected device is {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\n",
    "    env_name='CartPole-v1', \n",
    "    from_pixels=True, \n",
    "    pixels_only=False,\n",
    "    device=device\n",
    ")\n",
    "env = TransformedEnv(\n",
    "    env=base_env,\n",
    "    transform=Compose(\n",
    "        ToTensorImage(),\n",
    "        GrayScale(),\n",
    "        Resize(84,84),\n",
    "        StepCounter(),\n",
    "        InitTracker(),\n",
    "        RewardScaling(loc=0.0, scale=0.1),\n",
    "        ObservationNorm(standard_normal=True, in_keys=['pixels'])\n",
    "    )\n",
    ")\n",
    "env.transform[-1].init_stats(1000, reduce_dim=[0, 1, 2], cat_dim=0, keep_dims=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c9b0a075c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "td = env.reset()\n",
    "plt.imshow(td['pixels'].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = TensorDictModule(\n",
    "    ConvNet(\n",
    "        num_cells=[32, 32, 64],\n",
    "        squeeze_output=True,\n",
    "        aggregator_class=nn.AdaptiveAvgPool2d,\n",
    "        aggregator_kwargs={\"output_size\": (1, 1)},\n",
    "        device=device,\n",
    "    ),\n",
    "    in_keys=[\"pixels\"],\n",
    "    out_keys=[\"embed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = feature(env.reset())['embed'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMModule(\n",
    "    input_size=n_cells,\n",
    "    hidden_size=128,\n",
    "    device=device,\n",
    "    in_key='embed',\n",
    "    out_key='embed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['embed', 'recurrent_state_h', 'recurrent_state_c', 'is_init'],\n",
       " ['embed', ('next', 'recurrent_state_h'), ('next', 'recurrent_state_c')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('in_keys: '), lstm.out_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformedEnv(\n",
       "    env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu),\n",
       "    transform=Compose(\n",
       "            ToTensorImage(keys=['pixels']),\n",
       "            GrayScale(keys=['pixels']),\n",
       "            Resize(w=84, h=84, interpolation=InterpolationMode.BILINEAR, keys=['pixels']),\n",
       "            StepCounter(keys=[]),\n",
       "            InitTracker(keys=[]),\n",
       "            RewardScaling(loc=0.0000, scale=0.1000, keys=['reward']),\n",
       "            ObservationNorm(keys=['pixels']),\n",
       "            TensorDictPrimer(primers=Composite(\n",
       "                recurrent_state_h: UnboundedContinuous(\n",
       "                    shape=torch.Size([1, 128]),\n",
       "                    space=ContinuousBox(\n",
       "                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                    device=cpu,\n",
       "                    dtype=torch.float32,\n",
       "                    domain=continuous),\n",
       "                recurrent_state_c: UnboundedContinuous(\n",
       "                    shape=torch.Size([1, 128]),\n",
       "                    space=ContinuousBox(\n",
       "                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                    device=cpu,\n",
       "                    dtype=torch.float32,\n",
       "                    domain=continuous),\n",
       "                device=cpu,\n",
       "                shape=torch.Size([])), default_value={'recurrent_state_h': 0.0, 'recurrent_state_c': 0.0}, random=None)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.append_transform(lstm.make_tensordict_primer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(\n",
    "    out_features=2,\n",
    "    num_cells=[64,],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp[-1].bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = TensorDictModule(\n",
    "    module=mlp,\n",
    "    in_keys=['embed'],\n",
    "    out_keys=['action_value']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=MLP(\n",
       "      (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['embed'],\n",
       "    out_keys=['action_value'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qval = QValueModule(action_space=None, spec=env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoch_policy = TensorDictSequential(feature, lstm, mlp, qval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_module = EGreedyModule(\n",
    "    annealing_num_steps=1_000_000, spec=env.action_spec, eps_init=0.2\n",
    ")\n",
    "stoch_policy = TensorDictSequential(\n",
    "    stoch_policy,\n",
    "    exploration_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TensorDictSequential(feature, lstm.set_recurrent_mode(True), mlp, qval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        embed: Tensor(shape=torch.Size([128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        is_init: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                recurrent_state_c: Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                recurrent_state_h: Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            batch_size=torch.Size([]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        pixels: Tensor(shape=torch.Size([1, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        recurrent_state_c: Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        recurrent_state_h: Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DQNLoss(policy, action_space=env.action_spec, delay_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = SoftUpdate(loss_fn, eps=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(env, stoch_policy, frames_per_batch=50, total_frames=200)\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(20_000), batch_size=4, prefetch=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:01<00:03, 43.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us print the first batch of data.\n",
      "Pay attention to the key names which will reflect what can be found in this data structure, in particular: the output of the QValueModule (action_values, action and chosen_action_value),the 'is_init' key that will tell us if a step is initial or not, and the recurrent_state keys.\n",
      " TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([50, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([50, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([50]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([50]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        embed: Tensor(shape=torch.Size([50, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        is_init: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                is_init: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([50, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                pixels: Tensor(shape=torch.Size([50, 1, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                recurrent_state_c: Tensor(shape=torch.Size([50, 1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                recurrent_state_h: Tensor(shape=torch.Size([50, 1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([50]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([50, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pixels: Tensor(shape=torch.Size([50, 1, 84, 84]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        recurrent_state_c: Tensor(shape=torch.Size([50, 1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        recurrent_state_h: Tensor(shape=torch.Size([50, 1, 128]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([50, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([50]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(data\u001b[38;5;241m.\u001b[39mnumel())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# it is important to pass data that is not flattened\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mrb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensordict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(utd):\n\u001b[0;32m     20\u001b[0m     s \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:1219\u001b[0m, in \u001b[0;36mTensorDictReplayBuffer.extend\u001b[1;34m(self, tensordicts)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordicts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mndim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m-> 1219\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordicts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;66;03m# TODO: to be usable directly, the indices should be flipped but the issue\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;66;03m#  is that just doing this results in indices that are not sorted like the original data\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;66;03m#  so the actualy indices will have to be used on the _storage directly (not on the buffer)\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_index_in_td(tensordicts, index)\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:606\u001b[0m, in \u001b[0;36mReplayBuffer._extend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_extend \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    605\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transpose(data)\n\u001b[1;32m--> 606\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39mextend(index)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\writers.py:306\u001b[0m, in \u001b[0;36mTensorDictRoundRobinWriter.extend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    298\u001b[0m     data\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    300\u001b[0m         expand_as_right(\n\u001b[0;32m    301\u001b[0m             torch\u001b[38;5;241m.\u001b[39mas_tensor(index, device\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), data\n\u001b[0;32m    302\u001b[0m         ),\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Replicate index requires the shape of the storage to be known\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# Other than that, a \"flat\" (1d) index is ok to write the data\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replicate_index(index)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39m_attached_entities:\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\_utils.py:394\u001b[0m, in \u001b[0;36mimplement_for.__call__.<locals>._lazy_call_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# first time we call the function, we also do the replacement.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# This will cause the imports to occur only during the first call to fn\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delazify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:718\u001b[0m, in \u001b[0;36mTensorStorage.set\u001b[1;34m(self, cursor, data, set_cursor)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cursor, INT_CLASSES):\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_tensor_collection(data):\n\u001b[1;32m--> 718\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], data))\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\storages.py:1175\u001b[0m, in \u001b[0;36mLazyMemmapStorage._init\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1173\u001b[0m out \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1174\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexpand(max_size_along_dim0(data\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m-> 1175\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscratch_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexistsok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexistsok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torchrl_logger\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m   1178\u001b[0m         out\u001b[38;5;241m.\u001b[39mitems(\n\u001b[0;32m   1179\u001b[0m             include_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1184\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\tensordict\\base.py:4484\u001b[0m, in \u001b[0;36mTensorDictBase.memmap_like\u001b[1;34m(self, prefix, copy_existing, existsok, num_threads, return_early, share_non_tensor)\u001b[0m\n\u001b[0;32m   4480\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m TensorDictFuture(futures, result)\n\u001b[0;32m   4481\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m   4482\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mempty((), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   4483\u001b[0m )\n\u001b[1;32m-> 4484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_memmap_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4487\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_non_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_non_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexistsok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexistsok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlock_()\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\tensordict\\_td.py:2589\u001b[0m, in \u001b[0;36mTensorDict._memmap_\u001b[1;34m(self, prefix, copy_existing, executor, futures, inplace, like, share_non_tensor, existsok)\u001b[0m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m executor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2589\u001b[0m         \u001b[43m_populate_memmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2590\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexistsok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexistsok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2599\u001b[0m         futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   2600\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2601\u001b[0m                 _populate_memmap,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2609\u001b[0m             )\n\u001b[0;32m   2610\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\tensordict\\_td.py:4464\u001b[0m, in \u001b[0;36m_populate_memmap\u001b[1;34m(dest, value, key, copy_existing, prefix, like, existsok)\u001b[0m\n\u001b[0;32m   4462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4463\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4464\u001b[0m memmap_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mMemoryMappedTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexistsok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexistsok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4472\u001b[0m dest\u001b[38;5;241m.\u001b[39m_tensordict[key] \u001b[38;5;241m=\u001b[39m memmap_tensor\n\u001b[0;32m   4473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memmap_tensor\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\tensordict\\memmap.py:214\u001b[0m, in \u001b[0;36mMemoryMappedTensor.from_tensor\u001b[1;34m(cls, input, filename, existsok, copy_existing, copy_data, shape)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# assume integer\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m shape_numel\n\u001b[1;32m--> 214\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[43m_FileHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    216\u001b[0m     func_offset_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    217\u001b[0m         torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_nested_compute_contiguous_strides_offsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Reza\\Documents\\Projects\\Github_Projects\\.venv\\Lib\\site-packages\\tensordict\\memmap.py:935\u001b[0m, in \u001b[0;36m_FileHandler.__init__\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m    934\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpym-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand))\n\u001b[1;32m--> 935\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mmmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _winapi\u001b[38;5;241m.\u001b[39mGetLastError() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] The paging file is too small for this operation to complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:20<00:03, 43.83it/s]"
     ]
    }
   ],
   "source": [
    "utd = 16\n",
    "pbar = tqdm.tqdm(total=collector.total_frames)\n",
    "longest = 0\n",
    "\n",
    "traj_lens = []\n",
    "for i, data in enumerate(collector):\n",
    "    if i == 0:\n",
    "        print(\n",
    "            \"Let us print the first batch of data.\\nPay attention to the key names \"\n",
    "            \"which will reflect what can be found in this data structure, in particular: \"\n",
    "            \"the output of the QValueModule (action_values, action and chosen_action_value),\"\n",
    "            \"the 'is_init' key that will tell us if a step is initial or not, and the \"\n",
    "            \"recurrent_state keys.\\n\",\n",
    "            data,\n",
    "        )\n",
    "    pbar.update(data.numel())\n",
    "    # it is important to pass data that is not flattened\n",
    "    rb.extend(data.unsqueeze(0).to_tensordict().cpu())\n",
    "    for _ in range(utd):\n",
    "        s = rb.sample().to(device, non_blocking=True)\n",
    "        loss_vals = loss_fn(s)\n",
    "        loss_vals[\"loss\"].backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    longest = max(longest, data[\"step_count\"].max().item())\n",
    "    pbar.set_description(\n",
    "        f\"steps: {longest}, loss_val: {loss_vals['loss'].item(): 4.4f}, action_spread: {data['action'].sum(0)}\"\n",
    "    )\n",
    "    exploration_module.step(data.numel())\n",
    "    updater.step()\n",
    "\n",
    "    with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "        rollout = env.rollout(10000, stoch_policy)\n",
    "        traj_lens.append(rollout.get((\"next\", \"step_count\")).max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traj_lens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtraj_lens\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(traj_lens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'traj_lens' is not defined"
     ]
    }
   ],
   "source": [
    "if traj_lens:\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    plt.plot(traj_lens)\n",
    "    plt.xlabel(\"Test collection\")\n",
    "    plt.title(\"Test trajectory lengths\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
